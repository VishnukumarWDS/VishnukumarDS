{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d74cdf-940e-4582-9bf1-9c05b1d2a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Overfitting occurs when a machine learning model learns the noise in the training data, resulting in a model that is overly complex and performs well on the training data but poorly on new, unseen data. The consequence of overfitting is poor generalization performance of the model, which means it will not be able to make accurate predictions on new data. On the other hand, underfitting occurs when the model is too simple and cannot capture the underlying patterns in the data. This leads to poor performance on both the training and test data. Underfitting can be mitigated by increasing model complexity, using more advanced algorithms, or adding more relevant features to the data. Overfitting can be mitigated by using techniques such as regularization, cross-validation, or increasing the amount of training data.\n",
    "\n",
    "Q2: One way to reduce overfitting is to use regularization techniques such as L1 and L2 regularization. These techniques add a penalty term to the cost function, which encourages the model to have smaller weights and reduces overfitting. Another way is to use early stopping, which stops the training process when the performance on the validation set stops improving. This prevents the model from continuing to learn the noise in the training data.\n",
    "\n",
    "Q3: Underfitting occurs when the model is too simple to capture the underlying patterns in the data. This can happen in scenarios where the data is too noisy or the model is too simple. For example, if we try to fit a linear model to a dataset with a non-linear relationship between the input and output variables, the model will underfit and perform poorly.\n",
    "\n",
    "Q4: The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between model complexity, bias, and variance. Bias refers to the error due to the simplifying assumptions made by the model, while variance refers to the error due to the model's sensitivity to small fluctuations in the training data. A model with high bias is too simple and cannot capture the underlying patterns in the data, while a model with high variance is too complex and learns the noise in the training data. The goal is to find a model with a balance between bias and variance that can generalize well to new data.\n",
    "\n",
    "Q5: Common methods for detecting overfitting and underfitting include analyzing the learning curve, using cross-validation, and examining the performance of the model on the training and test data. A learning curve plots the performance of the model on the training and test data as a function of the amount of training data. If the model has high variance, the performance on the test data will plateau, while if it has high bias, the performance on both the training and test data will be poor. Cross-validation involves splitting the data into multiple training and validation sets and evaluating the model's performance on each set. If the model performs well on the training data but poorly on the validation data, it is likely overfitting.\n",
    "\n",
    "Q6: Bias refers to the error due to the simplifying assumptions made by the model, while variance refers to the error due to the model's sensitivity to small fluctuations in the training data. A high bias model is too simple and cannot capture the underlying patterns in the data, while a high variance model is too complex and learns the noise in the training data. A high bias model will perform poorly on both the training and test data, while a high variance model will perform well on the training data but poorly on the test data.\n",
    "\n",
    "Q7: Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function that encourages the model to have smaller weights. Common regularization techniques include L1 and L2 regularization, which add a penalty term to the cost function that is proportional to the absolute or squared value of the weights, respectively. Another technique"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
