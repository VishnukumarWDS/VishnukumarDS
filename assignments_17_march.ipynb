{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc0af54-f233-4203-982f-343825a79ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad988aca-a738-4d94-8ff5-bd4c21d8c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing values in a dataset refer to the absence of values or data points in one or more fields of a dataset. Handling missing values is crucial because it can lead to biased or inaccurate results if not dealt with properly. Some algorithms that are not affected by missing values include Decision Trees, Random Forest, and Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d37f0-5cba-4981-8437-982fcee8cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125914d9-415d-4180-9d73-a13f7c1032c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a) Deleting: Dropping the rows or columns that contain missing values.\n",
    "df.dropna() #drops rows with any missing values\n",
    "df.dropna(axis=1) #drops columns with any missing values\n",
    "\n",
    "b) Imputation: Filling in the missing values with a substitute value.\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[['column_name']] = imputer.fit_transform(df[['column_name']])\n",
    "\n",
    "c) Prediction: Using machine learning algorithms to predict the missing values based on other variables in the dataset.\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imputer = IterativeImputer()\n",
    "df[['column_name']] = imputer.fit_transform(df[['column_name']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e2670-7550-436b-8249-a0dd59d2f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59137e5-9fea-44a2-8d09-990a459d4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imbalanced data refers to a situation in which the distribution of classes in a dataset is uneven, with one class having significantly fewer observations than the others. If imbalanced data is not handled properly, it can lead to biased or inaccurate model performance, where the model may predict the majority class accurately, but the minority class is ignored or under-predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8e4cb-b52d-4c4d-972b-29f6c5873994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c967af-787b-438b-aa7f-d1694dd696ab",
   "metadata": {},
   "outputs": [],
   "source": [
    " Up-sampling and down-sampling are techniques used to balance imbalanced datasets.\n",
    "\n",
    "Up-sampling involves adding more instances of the minority class to the dataset to increase the representation of that class.\n",
    "\n",
    "Down-sampling involves reducing the number of instances in the majority class to balance the dataset.\n",
    "\n",
    "An example of when up-sampling and down-sampling are required is in credit card fraud detection, where the occurrence of fraud is rare compared to non-fraudulent transactions. In such a scenario, up-sampling the minority class can help the model learn patterns that distinguish fraudulent transactions from non-fraudulent ones. Down-sampling the majority class may be required in cases where the dataset is too large and computationally intensive, making it difficult to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74613993-ed05-4f8f-a721-e21d2738d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q5: What is data Augmentation? Explain SMOTE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18fc368-593f-417a-9306-64653362e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data augmentation is a technique used to increase the size of a dataset by creating new synthetic examples from the existing data. SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation technique that generates synthetic examples of the minority class by interpolating new data points between existing ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eee740-15af-4a99-8e1d-c88e53e5dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c027184-68bd-4e29-98eb-87c9af9cc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "Outliers in a dataset are extreme values that deviate significantly from other values in the dataset. Handling outliers is important because they can skew the results of statistical analyses and machine learning models. Outliers can be detected using statistical methods like the Z-score or visual methods like box plots or scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fab8f0-6665-437f-94e9-ed80d87c349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504b803b-91ee-42b1-b057-3c658452b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some techniques for handling missing data in customer data analysis include imputing missing values with mean or median values, using predictive modeling to fill in missing data, or removing the rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd2bff-9cd5-47ef-a985-040e22d84a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88539185-4237-4612-84ce-e07a238235d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "To determine if the missing data is missing at random or if there is a pattern to the missing data, some strategies include analyzing the patterns of missing values across the dataset, looking for correlations between missing data and other variables in the dataset, or using statistical tests like Little's MCAR test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f9f19-e2cd-4e39-bdc1-636540797af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328ca5e-02be-4480-99f6-02bde4e12cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    " Strategies for evaluating the performance of machine learning models on imbalanced datasets include using metrics like precision, recall, F1 score, or AUC-ROC curve. Additionally, techniques like resampling, modifying the class weights, or using different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823bb05c-b5a0-44ad-89b8-9f0de31deada",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5425b0-6a01-42d7-a83c-d022a31d1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Under-Sampling: This involves randomly removing samples from the majority class until it is balanced with the minority class.\n",
    "from sklearn.utils import resample\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df['satisfaction'] == 'satisfied']\n",
    "minority_class = df[df['satisfaction'] == 'unsatisfied']\n",
    "# Downsample majority class\n",
    "downsampled_majority = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "# Combine minority class and downsampled majority class\n",
    "balanced_data = pd.concat([downsampled_majority, minority_class])\n",
    "\n",
    "Cluster-Centroids: This method creates new samples by finding centroids for the majority class and creating new samples based on the difference between the centroid and the minority class.\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "# Define the ClusterCentroids model\n",
    "cc = ClusterCentroids(random_state=42)\n",
    "# Resample the majority class\n",
    "X_resampled, y_resampled = cc.fit_resample(X, y)\n",
    "\n",
    "NearMiss: This method selects samples from the majority class that are closest to the minority class based on distance metrics.\n",
    "from imblearn.under_sampling import NearMiss\n",
    "# Define the NearMiss model\n",
    "nm = NearMiss(version=1, n_neighbors=3)\n",
    "# Resample the majority class\n",
    "X_resampled, y_resampled = nm.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2068a7a-4461-4569-8339-5dbe5d50e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f79b7f4-d129-4ae9-9348-97e4b555f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Over-Sampling: This involves randomly replicating samples from the minority class until it is balanced with the majority class.\n",
    "from sklearn.utils import resample\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df['rare_event'] == 0]\n",
    "minority_class = df[df['rare_event'] == 1]\n",
    "# Upsample minority class\n",
    "upsampled_minority = resample(minority_class, replace=True, n_samples=len(majority_class), random_state=42)\n",
    "# Combine majority class and upsampled minority class\n",
    "balanced_data = pd.concat([majority_class, upsampled_minority])\n",
    "\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): This method creates new synthetic samples for the minority class by finding its k-nearest neighbors and creating new samples based on the difference between the minority class and its neighbors.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Define the SMOTE model\n",
    "smote = SMOTE(random_state=42)\n",
    "# Resample the minority class\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "ADASYN (Adaptive Synthetic Sampling): This method is similar to SMOTE, but it creates more synthetic samples for the minority class near the decision boundary.\n",
    "from imblearn.over_sampling import ADASYN\n",
    "# Define the ADASYN model\n",
    "adasyn = ADASYN(random_state=42)\n",
    "# Resample the minority class\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099659c-70cf-446b-bb21-e82219b15cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
