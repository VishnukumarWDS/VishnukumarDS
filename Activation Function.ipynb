{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76101edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Activation function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dcde40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is an activation function in the context of artificial neural networks?\n",
    "A1. An activation function in artificial neural networks determines the output of a neuron given its input. It introduces non-linearity to the network, allowing it to learn complex patterns and relationships in the data.\n",
    "\n",
    "Q2. What are some common types of activation functions used in neural networks?\n",
    "A2. Common activation functions include sigmoid, hyperbolic tangent (tanh), rectified linear unit (ReLU), and softmax.\n",
    "\n",
    "Q3. How do activation functions affect the training process and performance of a neural network?\n",
    "A3. Activation functions influence the network's ability to approximate complex functions and learn representations of the data. They also impact the gradient flow during backpropagation, which affects the speed and stability of training.\n",
    "\n",
    "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "A4. The sigmoid function squashes the input into the range [0, 1], making it useful for binary classification tasks. Its advantages include smoothness and interpretation as probabilities. However, it suffers from the vanishing gradient problem and outputs close to 0 or 1 can cause saturation, slowing down learning.\n",
    "\n",
    "Q5. What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "A5. The ReLU function returns 0 for negative inputs and the input value for positive inputs. Unlike sigmoid, ReLU is not bounded, allowing for faster learning due to reduced likelihood of saturation.\n",
    "\n",
    "Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "A6. ReLU avoids the vanishing gradient problem, accelerates convergence, and is computationally efficient to evaluate, making it a preferred choice in many neural network architectures.\n",
    "\n",
    "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "A7. Leaky ReLU allows a small, non-zero gradient for negative inputs, preventing the neuron from being completely inactive and addressing the vanishing gradient problem encountered with traditional ReLU activations.\n",
    "\n",
    "Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "A8. The softmax function converts the raw scores of a network's output layer into probabilities, facilitating multi-class classification tasks. It's commonly used in the output layer of neural networks for tasks where only one class can be present in the prediction.\n",
    "\n",
    "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "A9. The tanh function is similar to the sigmoid but squashes the input into the range [-1, 1]. It addresses the saturation problem of the sigmoid function by centering the output around zero, allowing for faster convergence in some cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
