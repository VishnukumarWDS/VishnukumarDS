{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc7d18-0132-4802-902b-b728ef486adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed39e65f-5440-44a9-8584-92cd7ee10b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    " Min-Max scaling is a data preprocessing technique used to scale numerical data to a specific range, usually between 0 and 1. It works by subtracting the minimum value of the data and then dividing by the range of the data. This technique is useful for data normalization and can help improve the performance of machine learning algorithms.\n",
    "\n",
    "For example, suppose you have a dataset of student scores on an exam ranging from 60 to 100. You can use Min-Max scaling to scale the scores to a range of 0 to 1 by subtracting 60 from each score and then dividing by 40 (the range of the data). So a score of 70 would be scaled to (70-60)/40 = 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7845bd5d-0707-4a3d-847b-a7ea1ead103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d35c54-2b77-460b-8a77-3ede13a3d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Unit Vector technique in feature scaling is a method of scaling data that scales the data so that each data point lies on a unit vector. This technique scales the data by dividing each data point by the Euclidean length of the vector. The resulting data points lie on the surface of a unit sphere.\n",
    "\n",
    "Compared to Min-Max scaling, the Unit Vector technique does not preserve the original range of the data. Instead, it scales the data based on their direction. This technique is useful for algorithms that require the data to have equal weights or magnitudes.\n",
    "\n",
    "For example, suppose you have a dataset of customer reviews for a product, where each review is represented as a vector of words. You can use the Unit Vector technique to scale each review vector so that they have equal weights and lie on the surface of a unit sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a83d1-03d5-486d-85c1-29aa6e6b003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4072abc-c805-4746-855b-63609c498815",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA (Principle Component Analysis) is a technique used in machine learning to reduce the dimensionality of a dataset by identifying the most important features or components. It works by transforming the original features into a new set of linearly uncorrelated features called principal components. Each principal component captures a different source of variation in the data, and the components are ordered by the amount of variance they explain.\n",
    "\n",
    "PCA is commonly used in dimensionality reduction to reduce the number of features in a dataset, making it easier to analyze and visualize. It can also help improve the performance of machine learning algorithms by reducing overfitting and simplifying the model.\n",
    "\n",
    "For example, suppose you have a dataset of customer purchase data, where each data point represents a customer's purchase history. You can use PCA to reduce the dimensionality of the data by identifying the most important features, such as the total amount spent, the number of purchases, and the types of items purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e202b4-e70f-42da-8cbe-a83099b17736",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587b9da-7b56-4e5d-a45f-2c196e7f4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA can be used for feature extraction, which involves transforming the original features of a dataset into a smaller set of features that capture the most important information. Feature extraction is often used in machine learning to reduce the dimensionality of a dataset and improve the performance of the model.\n",
    "\n",
    "PCA works by identifying the most important features or components in a dataset and then transforming the original features into a new set of linearly uncorrelated features called principal components. These principal components can be used as the new features for the dataset.\n",
    "\n",
    "For example, suppose you have a dataset of images, where each image is represented as a matrix of pixel values. You can use PCA to extract the most important features or components from the images, such as the edges, textures, and shapes. These principal components can be used as the new features for the dataset, which can then be used to train a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ccf88-ce82-4e2d-bb0b-89900ce2e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc29b76-4d26-4b8b-a2ab-39f758d6b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "To use Min-Max scaling to preprocess the data in a food delivery service recommendation system, the following steps can be taken:\n",
    "\n",
    "Identify the features that need to be scaled. In this case, the features are price, rating, and delivery time.\n",
    "\n",
    "Determine the minimum and maximum values for each feature in the dataset. For example, the minimum price might be $5, and the maximum price might be $50.\n",
    "\n",
    "Use the Min-Max scaling formula to scale each feature value to a range of 0 to 1. The formula is:\n",
    "\n",
    "Scaled value = (Original value - Minimum value) / (Maximum value - Minimum value)\n",
    "\n",
    "For example, if the original price is $10, the minimum price is $5, and the maximum price is $50, the scaled value would be:\n",
    "\n",
    "Scaled price = ($10 - $5) / ($50 - $5) = 0.167\n",
    "\n",
    "Repeat step 3 for each feature in the dataset.\n",
    "\n",
    "The resulting dataset will have all features scaled to a range of 0 to 1, which can be used as input to a machine learning algorithm for building the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd33f4-8389-44aa-aa51-33f9c72bb673",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8280a-376d-4370-842b-522927d9deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "To use PCA to reduce the dimensionality of the dataset in a stock price prediction model, the following steps can be taken:\n",
    "\n",
    "First, preprocess the dataset to remove any missing values, outliers, or other data quality issues.\n",
    "\n",
    "Identify the features that need to be reduced. In this case, the features are the company financial data and market trends.\n",
    "\n",
    "Calculate the covariance matrix for the features in the dataset.\n",
    "\n",
    "Use the covariance matrix to calculate the principal components of the dataset.\n",
    "\n",
    "Determine the number of principal components to keep. This can be done by looking at the explained variance of each component and selecting the top components that account for a large percentage of the total variance in the dataset. This can be done using a scree plot or by inspecting the cumulative explained variance.\n",
    "\n",
    "Project the original dataset onto the reduced set of principal components to create a new dataset with reduced dimensionality.\n",
    "\n",
    "The resulting reduced dataset can be used as input to a machine learning algorithm for predicting stock prices.\n",
    "\n",
    "By reducing the dimensionality of the dataset using PCA, the model can be more efficient and accurate because it focuses on the most important features that explain the variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67d357-063a-4a0f-ae37-fdd14ecbda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bedbf76-7b08-421c-8ebf-66c7ad1c32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "To transform the categorical data into numerical data in the dataset for predicting customer churn, the following encoding techniques can be used:\n",
    "\n",
    "One-hot encoding: This technique is used to encode categorical features with no ordinal relationship, such as gender and contract type.\n",
    "Step-by-step implementation of one-hot encoding:\n",
    "\n",
    "a. Create a binary column for each unique value in the categorical feature.\n",
    "b. For each row in the dataset, assign a 1 to the binary column that corresponds to the value in the original categorical feature, and assign 0 to all other binary columns.\n",
    "\n",
    "For example, if the original gender column contains 'Male' and 'Female', the one-hot encoding would create two new binary columns, 'Male' and 'Female'. If a row in the dataset has a value of 'Male' for gender, then the 'Male' column would be assigned a 1, and the 'Female' column would be assigned a 0.\n",
    "\n",
    "Label encoding: This technique is used to encode categorical features with an ordinal relationship, such as low, medium, and high tenure.\n",
    "Step-by-step implementation of label encoding:\n",
    "\n",
    "a. Assign a numerical label to each unique value in the categorical feature, based on the order of importance or hierarchy.\n",
    "b. For each row in the dataset, replace the value in the categorical feature with the corresponding numerical label.\n",
    "\n",
    "For example, if the original tenure column contains 'Low', 'Medium', and 'High', the label encoding would assign 'Low' a label of 1, 'Medium' a label of 2, and 'High' a label of 3. If a row in the dataset has a value of 'Medium' for tenure, then the value in the tenure column would be replaced with the corresponding label, 2.\n",
    "\n",
    "Overall, a combination of one-hot encoding and label encoding can be used to transform the categorical data into numerical data, depending on the type of categorical feature and the relationship between its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed81ee-6003-4d50-ad0e-de1796de4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a9ff5-c6ff-4bb2-8958-168dcc8d1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform feature extraction using PCA for a dataset containing the features [height, weight, age, gender, blood pressure], the following steps can be taken:\n",
    "\n",
    "Preprocess the dataset to remove any missing values, outliers, or other data quality issues.\n",
    "\n",
    "Standardize the dataset to ensure that each feature has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Calculate the covariance matrix for the features in the standardized dataset.\n",
    "\n",
    "Use the covariance matrix to calculate the principal components of the dataset.\n",
    "\n",
    "Determine the number of principal components to retain. This can be done by looking at the explained variance of each component and selecting the top components that account for a large percentage of the total variance in the dataset. A commonly used threshold is to retain principal components that explain at least 70-80% of the total variance in the dataset.\n",
    "\n",
    "Project the original dataset onto the reduced set of principal components to create a new dataset with reduced dimensionality.\n",
    "\n",
    "The number of principal components to retain would depend on the amount of variance that needs to be explained and the desired level of dimensionality reduction. In this case, since there are only 5 features, it is possible that all features are important for predicting the outcome variable, and reducing the dimensionality too much may result in loss of information. Therefore, it may be reasonable to retain at least 3-4 principal components that explain a high percentage of the total variance, such as 80-90%. However, the optimal number of principal components to retain ultimately depends on the specific dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb5e93-a170-41e4-804b-b1f296ef82cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
